{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "w7luYua5ppZr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist_binary(num_train=200, num_test=200, rng=np.random.RandomState(0)):\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x = np.concatenate([x_train, x_test], axis=0)\n",
        "    y = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "    idx = np.where((y == 0) | (y == 1))[0]\n",
        "    x, y = x[idx], y[idx]\n",
        "\n",
        "    x = x.astype(np.float32) / 255.0\n",
        "    x = np.expand_dims(x, -1)\n",
        "\n",
        "    y = (y == 1).astype(np.int32)  # Make it 0 or 1\n",
        "\n",
        "    indices = rng.choice(len(x), num_train + num_test, replace=False)\n",
        "    x, y = x[indices], y[indices]\n",
        "\n",
        "    x_train, x_test = x[:num_train], x[num_train:]\n",
        "    y_train, y_test = y[:num_train], y[num_train:]\n",
        "\n",
        "    return jnp.array(x_train), jax.nn.one_hot(y_train, 2), jnp.array(x_test), jax.nn.one_hot(y_test, 2)\n",
        "\n"
      ],
      "metadata": {
        "id": "uW_kdNrsqiIW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_mlp_params(rng_key):\n",
        "    keys = jax.random.split(rng_key, 4)\n",
        "    params = {\n",
        "        \"w1\": jax.random.normal(keys[0], (28 * 28, 128)) * 0.01,\n",
        "        \"b1\": jnp.zeros(128),\n",
        "        \"w2\": jax.random.normal(keys[1], (128, 2)) * 0.01,\n",
        "        \"b2\": jnp.zeros(2)\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def model_fn(params, x):\n",
        "    x_flat = x.reshape((x.shape[0], -1))\n",
        "    hidden = jax.nn.relu(jnp.dot(x_flat, params[\"w1\"]) + params[\"b1\"])\n",
        "    logits = jnp.dot(hidden, params[\"w2\"]) + params[\"b2\"]\n",
        "    return logits"
      ],
      "metadata": {
        "id": "R4d144dJqk52"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FGSM(model_fn, params, x, y, eps):\n",
        "  loss_fn = lambda x_in: optax.softmax_cross_entropy(model_fn(params, x_in), y).mean()\n",
        "  grad = jax.grad(loss_fn)(x)\n",
        "  adv_x = x + eps * jnp.sign(grad)\n",
        "  return jnp.clip(adv_x, 0.0, 1.0)\n",
        "\n",
        "\n",
        "\n",
        "def PGD(model_fn, params, x, y, eps=8/255, alpha=2/255, steps=5):\n",
        "  x_adv = x.copy()\n",
        "  loss_fn = lambda x_in: optax.softmax_cross_entropy(model_fn(params, x_in), y).mean()\n",
        "  for _ in range(steps):\n",
        "      grads = jax.grad(loss_fn)(x_adv)\n",
        "      x_adv = x_adv + alpha * jnp.sign(grads)\n",
        "      x_adv = jnp.clip(x_adv, x - eps, x + eps)\n",
        "      x_adv = jnp.clip(x_adv, 0.0, 1.0)\n",
        "  return x_adv\n",
        "\n",
        "def APGD(model_fn, params, x, y, eps=0.3, alpha=0.01, steps=10, seed=0):\n",
        "  loss_fn = lambda x_in: optax.softmax_cross_entropy(model_fn(params, x_in), y)\n",
        "  loss_fn_scalar =  lambda x_in: optax.softmax_cross_entropy(model_fn(params, x_in), y).mean()\n",
        "  x_adv = x.copy()\n",
        "  step_size = alpha\n",
        "  cur_loss = loss_fn(x_adv)\n",
        "  cur_loss_mean = cur_loss.mean()\n",
        "  best_loss = cur_loss\n",
        "  best_loss_mean = cur_loss_mean\n",
        "  best_adv = x_adv\n",
        "\n",
        "  for _ in range(steps):\n",
        "    grads = jax.grad(loss_fn_scalar)(x_adv)\n",
        "    x_adv = x_adv + alpha * jnp.sign(grads)\n",
        "\n",
        "    x_adv = jnp.clip(x_adv, x - eps, x + eps)\n",
        "    x_adv = jnp.clip(x_adv, 0.0, 1.0)\n",
        "\n",
        "    cur_loss = loss_fn(x_adv)\n",
        "    cur_loss_mean = cur_loss.mean()\n",
        "    if cur_loss_mean < best_loss_mean:\n",
        "        step_size = step_size * 0.75\n",
        "\n",
        "    update_mask = cur_loss > best_loss\n",
        "    update_mask = update_mask[:, None, None, None]\n",
        "\n",
        "    best_adv = jnp.where(update_mask, x_adv, best_adv)\n",
        "    best_loss = jnp.maximum(best_loss, cur_loss)\n",
        "    best_loss_mean = best_loss.mean()\n",
        "  return best_adv\n",
        "\n",
        "def MIM(model_fn, params, x, y, eps=8/255, alpha=2/255, steps=10, decay=1.0):\n",
        "  x_adv = x.copy()\n",
        "  momentum = jnp.zeros_like(x)\n",
        "  loss_fn = lambda x_in:optax.softmax_cross_entropy(model_fn(params, x_in), y).mean()\n",
        "  for _ in range(steps):\n",
        "\n",
        "      grad = jax.grad(loss_fn)(x_adv)\n",
        "      grad = grad / (jnp.mean(jnp.abs(grad)) + 1e-8)\n",
        "\n",
        "      momentum = decay * momentum + grad\n",
        "      x_adv = x_adv + alpha * jnp.sign(momentum)\n",
        "      x_adv = jnp.clip(x_adv, x - eps, x + eps)\n",
        "  return jnp.clip(x_adv, 0, 1)\n",
        "def SA(model_fn, x, y, eps=0.3, num_iters=3, p=0.05, seed=0):\n",
        "  key = jax.random.PRNGKey(seed)\n",
        "  adv_x = x.copy()\n",
        "  for i in range(num_iters):\n",
        "      cur_p = p * (1 - i / num_iters)\n",
        "      for idx in range(x.shape[0]):\n",
        "          img = adv_x[idx]\n",
        "          H, W, C = img.shape\n",
        "          area = int(cur_p * H * W)\n",
        "          side_len = max(1, int(jnp.sqrt(area)))\n",
        "\n",
        "          key, rand = jax.random.split(key)\n",
        "          x0 = jax.random.randint(rand, (), 0, H - side_len)\n",
        "          y0 = jax.random.randint(rand, (), 0, W - side_len)\n",
        "\n",
        "          key, rand = jax.random.split(key)\n",
        "          perturbation = jax.random.uniform(rand, (side_len, side_len, C), minval=-eps, maxval=eps)\n",
        "\n",
        "          img = img.at[x0:x0+side_len, y0:y0+side_len, :].add(perturbation)\n",
        "          img = jnp.clip(img, 0, 1)\n",
        "          adv_x = adv_x.at[idx].set(img)\n",
        "  return adv_x\n"
      ],
      "metadata": {
        "id": "zsSU496wqnYg"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(x_train, y_train, num_epochs=10, lr=0.1):\n",
        "    params = init_mlp_params(jax.random.PRNGKey(0))\n",
        "    optimizer = optax.sgd(lr)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    def loss_fn(params, x, y):\n",
        "        logits = model_fn(params, x)\n",
        "        return optax.softmax_cross_entropy(logits, y).mean()\n",
        "\n",
        "    @jax.jit\n",
        "    def update(params, opt_state, x, y):\n",
        "        loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        return new_params, opt_state, loss\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        params, opt_state, loss = update(params, opt_state, x_train, y_train)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "def evaluate(params, x, y):\n",
        "    logits = model_fn(params, x)\n",
        "    preds = jnp.argmax(logits, axis=1)\n",
        "    labels = jnp.argmax(y, axis=1)\n",
        "    return jnp.mean(preds == labels)\n"
      ],
      "metadata": {
        "id": "BEtbgiYcq7jX"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = load_mnist_binary()\n",
        "params = train_model(x_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq9EsCy2q_NE",
        "outputId": "1318c3b5-28e6-4658-b41b-2da9b0f733a8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6961\n",
            "Epoch 2, Loss: 0.6890\n",
            "Epoch 3, Loss: 0.6821\n",
            "Epoch 4, Loss: 0.6745\n",
            "Epoch 5, Loss: 0.6647\n",
            "Epoch 6, Loss: 0.6515\n",
            "Epoch 7, Loss: 0.6332\n",
            "Epoch 8, Loss: 0.6084\n",
            "Epoch 9, Loss: 0.5755\n",
            "Epoch 10, Loss: 0.5337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"FGSM attempt\")\n",
        "acc_clean = evaluate(params, x_test, y_test)\n",
        "print(f\"Clean accuracy: {acc_clean:.4f}\")\n",
        "\n",
        "x_adv = FGSM(model_fn, params, x_test, y_test, eps=0.2)\n",
        "acc_adv = evaluate(params, x_adv, y_test)\n",
        "\n",
        "asr = 1.0 - acc_adv\n",
        "robustness_gap = acc_clean - acc_adv\n",
        "\n",
        "print(f\"Adversarial accuracy: {acc_adv:.4f}\")\n",
        "print(f\"Attack Success Rate: {asr:.4f}\")\n",
        "print(f\"Robustness Gap: {robustness_gap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwCoayXCrrZl",
        "outputId": "4ded2390-df2b-451f-a16a-c1ed1a41bf97"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FGSM attempt\n",
            "Clean accuracy: 0.9950\n",
            "Adversarial accuracy: 0.3950\n",
            "Attack Success Rate: 0.6050\n",
            "Robustness Gap: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"MIM attempt\")\n",
        "acc_clean = evaluate(params, x_test, y_test)\n",
        "print(f\"Clean accuracy: {acc_clean:.4f}\")\n",
        "\n",
        "x_adv = MIM(model_fn, params, x_test, y_test, eps=0.2)\n",
        "acc_adv = evaluate(params, x_adv, y_test)\n",
        "\n",
        "asr = 1.0 - acc_adv\n",
        "robustness_gap = acc_clean - acc_adv\n",
        "\n",
        "print(f\"Adversarial accuracy: {acc_adv:.4f}\")\n",
        "print(f\"Attack Success Rate: {asr:.4f}\")\n",
        "print(f\"Robustness Gap: {robustness_gap:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23KdFsMerEPF",
        "outputId": "e0b6a980-c7ad-4d91-b172-84eaac1a5aaa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MIM attempt\n",
            "Clean accuracy: 0.9950\n",
            "Adversarial accuracy: 0.9650\n",
            "Attack Success Rate: 0.0350\n",
            "Robustness Gap: 0.0300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"PGD attempt\")\n",
        "acc_clean = evaluate(params, x_test, y_test)\n",
        "print(f\"Clean accuracy: {acc_clean:.4f}\")\n",
        "\n",
        "x_adv = PGD(model_fn, params, x_test, y_test, eps=0.2)\n",
        "acc_adv = evaluate(params, x_adv, y_test)\n",
        "\n",
        "asr = 1.0 - acc_adv\n",
        "robustness_gap = acc_clean - acc_adv\n",
        "\n",
        "print(f\"Adversarial accuracy: {acc_adv:.4f}\")\n",
        "print(f\"Attack Success Rate: {asr:.4f}\")\n",
        "print(f\"Robustness Gap: {robustness_gap:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pnk7BXisPL3",
        "outputId": "cd205b2a-bb30-4838-ffe3-d49e2acef806"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PGD attempt\n",
            "Clean accuracy: 0.9950\n",
            "Adversarial accuracy: 0.9850\n",
            "Attack Success Rate: 0.0150\n",
            "Robustness Gap: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"SA attempt\")\n",
        "acc_clean = evaluate(params, x_test, y_test)\n",
        "print(f\"Clean accuracy: {acc_clean:.4f}\")\n",
        "x_adv = SA(model_fn, x_test, y_test, eps=0.2)\n",
        "acc_adv = evaluate(params, x_adv, y_test)\n",
        "\n",
        "asr = 1.0 - acc_adv\n",
        "robustness_gap = acc_clean - acc_adv\n",
        "\n",
        "print(f\"Adversarial accuracy: {acc_adv:.4f}\")\n",
        "print(f\"Attack Success Rate: {asr:.4f}\")\n",
        "print(f\"Robustness Gap: {robustness_gap:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-2CqnTVsWYF",
        "outputId": "68c892ec-d82a-4783-cc9c-858cd3177042"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SA attempt\n",
            "Clean accuracy: 0.9950\n",
            "Adversarial accuracy: 0.9950\n",
            "Attack Success Rate: 0.0050\n",
            "Robustness Gap: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"APGD attempt\")\n",
        "acc_clean = evaluate(params, x_test, y_test)\n",
        "print(f\"Clean accuracy: {acc_clean:.4f}\")\n",
        "#def     APGD(model_fn, params, x, y, eps=0.3, alpha=0.01, num_steps=10, seed=0)\n",
        "x_adv = APGD(model_fn, params, x_test, y_test, steps=3)\n",
        "acc_adv = evaluate(params, x_adv, y_test)\n",
        "\n",
        "asr = 1.0 - acc_adv\n",
        "robustness_gap = acc_clean - acc_adv\n",
        "\n",
        "print(f\"Adversarial accuracy: {acc_adv:.4f}\")\n",
        "print(f\"Attack Success Rate: {asr:.4f}\")\n",
        "print(f\"Robustness Gap: {robustness_gap:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stCyBBERG_Jv",
        "outputId": "113f7c7a-a332-4b6a-af6b-2ab19b6b8626"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "APGD attempt\n",
            "Clean accuracy: 0.9950\n",
            "Adversarial accuracy: 0.9900\n",
            "Attack Success Rate: 0.0100\n",
            "Robustness Gap: 0.0050\n"
          ]
        }
      ]
    }
  ]
}