{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "muaxV80GjGNH",
      "metadata": {
        "id": "muaxV80GjGNH"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d768ed5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mnist_data(n_train, n_test, binary=True):\n",
        "    \"\"\"Load and preprocess MNIST dataset.\n",
        "    \n",
        "    Args:\n",
        "        n_train: Number of training samples\n",
        "        n_test: Number of test samples\n",
        "        binary: If True, only use digits 0 and 1\n",
        "    \"\"\"\n",
        "    # Load MNIST\n",
        "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "    X = X.reshape(-1, 28, 28)\n",
        "    \n",
        "    if binary:\n",
        "        # Filter for digits 0 and 1\n",
        "        mask = (y == '0') | (y == '1')\n",
        "        X, y = X[mask], y[mask].astype(float)\n",
        "    else:\n",
        "        # Convert labels to float\n",
        "        y = y.astype(float)\n",
        "    \n",
        "    # Normalize pixel values to [0, 1]\n",
        "    X = X / 255.0\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, train_size=n_train, test_size=n_test, stratify=y)\n",
        "    \n",
        "    # Create patches\n",
        "    X_train_patches = create_patches(X_train)\n",
        "    X_test_patches = create_patches(X_test)\n",
        "    \n",
        "    return (\n",
        "        X_train_patches,\n",
        "        y_train.reshape(-1, 1),\n",
        "        X_test_patches,\n",
        "        y_test.reshape(-1, 1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e2438d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_qvit(n_train, n_test, n_epochs):\n",
        "    # Load data\n",
        "    x_train, y_train, x_test, y_test = load_mnist_data(n_train, n_test)\n",
        "\n",
        "    # Initialize model and parameters (S=16 for 4x4 patches)\n",
        "    model = QSANN_image_classifier(S=16, n=4, Denc=2, D=1, num_layers=1)\n",
        "    params = init_params(S=16, n=4, Denc=2, D=1, num_layers=1)\n",
        "    \n",
        "    # Define optimizer with same learning rate as PyTorch\n",
        "    optimizer = optax.adam(learning_rate=0.01)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # Create arrays to store metrics\n",
        "    train_cost_epochs = []\n",
        "    test_cost_epochs = []\n",
        "    train_acc_epochs = []\n",
        "    test_acc_epochs = []\n",
        "\n",
        "    # Loss function\n",
        "    def loss_fn(p, x, y):\n",
        "        y_pred = model(x, p)\n",
        "        return binary_cross_entropy(y, y_pred), y_pred\n",
        "\n",
        "    # JIT-compiled update step\n",
        "    @jax.jit\n",
        "    def update_step(params, opt_state, x_train, y_train, x_test, y_test):\n",
        "        # Get both value and gradient, along with model predictions\n",
        "        (loss_val, y_pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, x_train, y_train)\n",
        "        updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "        # Compute metrics\n",
        "        train_acc = accuracy(y_train, y_pred)\n",
        "        test_loss, test_acc = evaluate(model, new_params, x_test, y_test)\n",
        "\n",
        "        return new_params, new_opt_state, loss_val, train_acc, test_loss, test_acc\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        params, opt_state, train_cost, train_acc, test_cost, test_acc = update_step(\n",
        "            params, opt_state, x_train, y_train, x_test, y_test\n",
        "        )\n",
        "        \n",
        "        # Store metrics\n",
        "        train_cost_epochs.append(float(train_cost))\n",
        "        train_acc_epochs.append(float(train_acc))\n",
        "        test_cost_epochs.append(float(test_cost))\n",
        "        test_acc_epochs.append(float(test_acc))\n",
        "        \n",
        "        # Print progress every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Train Size: {n_train}, Epoch: {epoch + 1}/{n_epochs}, \"\n",
        "                  f\"Train Loss: {train_cost:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "                  f\"Test Loss: {test_cost:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    return dict(\n",
        "        n_train=[n_train] * n_epochs,\n",
        "        step=np.arange(1, n_epochs + 1, dtype=int),\n",
        "        train_cost=train_cost_epochs,\n",
        "        train_acc=train_acc_epochs,\n",
        "        test_cost=test_cost_epochs,\n",
        "        test_acc=test_acc_epochs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02526d88",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, params, x, y):\n",
        "    y_pred = model(x, params)\n",
        "    loss = binary_cross_entropy(y, y_pred)\n",
        "    acc = accuracy(y, y_pred)\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d686d4",
      "metadata": {},
      "source": [
        "## CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc5a8e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_cifar_data(n_train, n_test, batch_size, binary=True, augment=True):\n",
        "    \"\"\"Load and preprocess CIFAR-10 dataset with optional data augmentation.\n",
        "    Returns a batched and shuffled tf.data.Dataset for training, and JAX arrays for testing.\n",
        "    \"\"\"\n",
        "    # Load CIFAR-10\n",
        "    (X_train_full, y_train_full), (X_test_full, y_test_full) = tf.keras.datasets.cifar10.load_data()\n",
        "    \n",
        "    if binary:\n",
        "        # Use only two classes (0: airplane, 1: automobile)\n",
        "        mask_train = (y_train_full[:, 0] == 0) | (y_train_full[:, 0] == 1)\n",
        "        mask_test = (y_test_full[:, 0] == 0) | (y_test_full[:, 0] == 1)\n",
        "        X_train_full = X_train_full[mask_train]\n",
        "        y_train_full = y_train_full[mask_train]\n",
        "        X_test_full = X_test_full[mask_test]\n",
        "        y_test_full = y_test_full[mask_test]\n",
        "        # Convert labels to binary (0 or 1)\n",
        "        y_train_full = (y_train_full == 1).astype(float)\n",
        "        y_test_full = (y_test_full == 1).astype(float)\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    X_train_full = X_train_full.astype('float32') / 255.0\n",
        "    X_test_full = X_test_full.astype('float32') / 255.0\n",
        "\n",
        "    # Select subset of data\n",
        "    indices_train = np.random.choice(len(X_train_full), n_train, replace=False)\n",
        "    indices_test = np.random.choice(len(X_test_full), n_test, replace=False)\n",
        "    X_train = X_train_full[indices_train]\n",
        "    y_train = y_train_full[indices_train]\n",
        "    X_test = X_test_full[indices_test]\n",
        "    y_test = y_test_full[indices_test]\n",
        "\n",
        "    # Data augmentation (only for training set)\n",
        "    if augment:\n",
        "        X_train_tf = tf.convert_to_tensor(X_train)\n",
        "        X_train_tf = tf.map_fn(augment_image, X_train_tf)\n",
        "        X_train = X_train_tf.numpy()\n",
        "\n",
        "    # Create patches\n",
        "    X_train_patches = create_patches(X_train)\n",
        "    X_test_patches = create_patches(X_test)\n",
        "    \n",
        "    # Create TensorFlow Dataset for training\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_patches, y_train))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=n_train, seed=42)  # Shuffle training data\n",
        "    train_dataset = train_dataset.batch(batch_size)\n",
        "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return (\n",
        "        train_dataset,  # Batched and shuffled training data\n",
        "        jnp.array(X_test_patches),\n",
        "        jnp.array(y_test)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "144df0ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_qvit(n_train, n_test, n_epochs, batch_size=128):\n",
        "    # Load data\n",
        "    train_dataset, x_test, y_test = load_cifar_data(n_train, n_test, batch_size)\n",
        "\n",
        "    # Initialize model and parameters\n",
        "    model = QSANN_image_classifier(S=64, n=5, Denc=2, D=1, num_layers=1)\n",
        "    params = init_params(S=64, n=5, Denc=2, D=1, num_layers=1)\n",
        "    \n",
        "    # Define optimizer with cosine annealing learning rate schedule\n",
        "    initial_lr = 0.003\n",
        "    lr_schedule = optax.cosine_decay_schedule(init_value=initial_lr, decay_steps=n_epochs)\n",
        "    optimizer = optax.adam(learning_rate=lr_schedule)\n",
        "\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # Create arrays to store metrics\n",
        "    train_costs = []\n",
        "    test_costs = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    steps = []\n",
        "\n",
        "    # Loss function\n",
        "    def loss_fn(p, x, y):\n",
        "        y_pred = model(x, p)\n",
        "        return binary_cross_entropy(y, y_pred), y_pred\n",
        "\n",
        "    # JIT-compiled update step for a single batch\n",
        "    @jax.jit\n",
        "    def update_batch(params, opt_state, x_batch, y_batch):\n",
        "        (loss_val, y_pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, x_batch, y_batch)\n",
        "        updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "        batch_acc = accuracy(y_batch, y_pred)\n",
        "        return new_params, new_opt_state, loss_val, batch_acc\n",
        "\n",
        "    # Training loop\n",
        "    current_params = params\n",
        "    current_opt_state = opt_state\n",
        "    start = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_acc = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for x_batch_tf, y_batch_tf in train_dataset:\n",
        "            # Convert TensorFlow tensors to JAX arrays\n",
        "            x_batch_jax = jnp.array(x_batch_tf.numpy())\n",
        "            y_batch_jax = jnp.array(y_batch_tf.numpy())\n",
        "            \n",
        "            current_params, current_opt_state, batch_loss, batch_acc = update_batch(\n",
        "                current_params, current_opt_state, x_batch_jax, y_batch_jax\n",
        "            )\n",
        "            epoch_train_loss += batch_loss\n",
        "            epoch_train_acc += batch_acc\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_epoch_train_loss = epoch_train_loss / num_batches\n",
        "        avg_epoch_train_acc = epoch_train_acc / num_batches\n",
        "\n",
        "        # Evaluate on test set at the end of each epoch\n",
        "        test_loss, test_acc = evaluate(model, current_params, x_test, y_test)\n",
        "        \n",
        "        # Store metrics\n",
        "        train_costs.append(float(avg_epoch_train_loss))\n",
        "        train_accs.append(float(avg_epoch_train_acc))\n",
        "        test_costs.append(float(test_loss))\n",
        "        test_accs.append(float(test_acc))\n",
        "        steps.append(epoch + 1)\n",
        "        \n",
        "        # Print progress every 10 epochs\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs} | \"\n",
        "                  f\"Train Loss: {avg_epoch_train_loss:.4f} | \"\n",
        "                  f\"Train Acc: {avg_epoch_train_acc:.4f} | \"\n",
        "                  f\"Test Loss: {test_loss:.4f} | \"\n",
        "                  f\"Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    # Create DataFrame with results\n",
        "    results_df = pd.DataFrame({\n",
        "        'step': steps,\n",
        "        'train_cost': train_costs,\n",
        "        'train_acc': train_accs,\n",
        "        'test_cost': test_costs,\n",
        "        'test_acc': test_accs,\n",
        "        'n_train': [n_train] * len(steps),\n",
        "        'batch_size': [batch_size] * len(steps)  # Add batch_size to results\n",
        "    })\n",
        "    \n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4b6ee9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, params, x, y):\n",
        "    y_pred = model(x, params)\n",
        "    loss = binary_cross_entropy(y, y_pred)\n",
        "    acc = accuracy(y, y_pred)\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e57348",
      "metadata": {},
      "source": [
        "## Digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e3463c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_digits_data(n_train, n_test):\n",
        "    digits = load_digits()\n",
        "    X, y = digits.data, digits.target\n",
        "    mask = (y == 0) | (y == 1)\n",
        "    X, y = X[mask], y[mask]\n",
        "    X = X / 16.0  # Normalize to [0, 1]\n",
        "    X = X.reshape(-1, 4, 16)\n",
        "    y = y.astype(jnp.float32)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, test_size=n_test)\n",
        "    return (\n",
        "        jnp.array(X_train),\n",
        "        jnp.array(y_train).reshape(-1, 1),\n",
        "        jnp.array(X_test),\n",
        "        jnp.array(y_test).reshape(-1, 1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2061177",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_qvit(n_train, n_test, n_epochs):\n",
        "    # Load data\n",
        "    x_train, y_train, x_test, y_test = load_digits_data(n_train, n_test)\n",
        "\n",
        "    # Initialize model and parameters\n",
        "    model = QSANN_image_classifier(S=4, n=4, Denc=2, D=1, num_layers=1)\n",
        "    params = init_params(S=4, n=4, Denc=2, D=1, num_layers=1)\n",
        "    \n",
        "    # Define optimizer with same learning rate as PyTorch\n",
        "    optimizer = optax.adam(learning_rate=0.01)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # Create arrays to store metrics\n",
        "    train_cost_epochs = []\n",
        "    test_cost_epochs = []\n",
        "    train_acc_epochs = []\n",
        "    test_acc_epochs = []\n",
        "\n",
        "    # Loss function\n",
        "    def loss_fn(p, x, y):\n",
        "        y_pred = model(x, p)\n",
        "        return binary_cross_entropy(y, y_pred), y_pred\n",
        "\n",
        "    # JIT-compiled update step\n",
        "    @jax.jit\n",
        "    def update_step(params, opt_state, x_train, y_train, x_test, y_test):\n",
        "        # Get both value and gradient, along with model predictions\n",
        "        (loss_val, y_pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, x_train, y_train)\n",
        "        updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "        # Compute metrics\n",
        "        train_acc = accuracy(y_train, y_pred)\n",
        "        test_loss, test_acc = evaluate(model, new_params, x_test, y_test)\n",
        "\n",
        "        return new_params, new_opt_state, loss_val, train_acc, test_loss, test_acc\n",
        "\n",
        "    # Training loop\n",
        "    start = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_start = time.time()\n",
        "        params, opt_state, train_cost, train_acc, test_cost, test_acc = update_step(\n",
        "            params, opt_state, x_train, y_train, x_test, y_test\n",
        "        )\n",
        "        \n",
        "        # Store metrics\n",
        "        train_cost_epochs.append(float(train_cost))\n",
        "        train_acc_epochs.append(float(train_acc))\n",
        "        test_cost_epochs.append(float(test_cost))\n",
        "        test_acc_epochs.append(float(test_acc))\n",
        "        \n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Train Size: {n_train}, Epoch: {epoch + 1}/{n_epochs}, \"\n",
        "                  f\"Train Loss: {train_cost:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "                  f\"Test Loss: {test_cost:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "    return dict(\n",
        "        n_train=[n_train] * n_epochs,\n",
        "        step=np.arange(1, n_epochs + 1, dtype=int),\n",
        "        train_cost=train_cost_epochs,\n",
        "        train_acc=train_acc_epochs,\n",
        "        test_cost=test_cost_epochs,\n",
        "        test_acc=test_acc_epochs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb437e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, params, x, y):\n",
        "    y_pred = model(x, params)\n",
        "    loss = binary_cross_entropy(y, y_pred)\n",
        "    acc = accuracy(y, y_pred)\n",
        "    return loss, acc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env-01",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
